---
---

@string{aps = {American Physical Society,}}

@article{10.1080/23273798.2023.2232903, 
bibtex_show={true},
year = {2023}, 
title = {{Stimulus repetition and sample size considerations in item-level representational similarity analysis}}, 
author = {Mazurchuk, Stephen and Conant, Lisa L. and Tong, Jia-Qing and Binder, Jeffrey R. and Fernandino, Leonardo}, 
journal = {Language, Cognition and Neuroscience}, 
issn = {2327-3798}, 
doi = {10.1080/23273798.2023.2232903}, 
abstract = {{In studies using representational similarity analysis (RSA) of fMRI data, the reliability of the neural representational dissimilarity matrix (RDM) is a limiting factor in the ability to detect neural correlates of a model. A common strategy for boosting neural RDM reliability is to employ repeated presentations of the stimulus set across imaging runs or sessions. However, little is known about how the benefits of stimulus repetition are affected by repetition suppression, or how they compare with the benefits of increasing the number of participants. We examined the effects of these design parameters in two large data sets where participants performed a semantic decision task on visually presented words. We found that reliability gains from stimulus repetition were strongly affected by repetition suppression, both within and across scanning sessions separated by multiple weeks. The results provide new insights into these experimental design choices, particularly for item-level RSA studies of semantic cognition.}}, 
pages = {1--12}, 
number = {ahead-of-print}, 
volume = {ahead-of-print}, 
keywords = {}
}

@article{10.1523/jneurosci.1243-21.2022, 
bibtex_show={true},
year = {2022}, 
title = {{A Distributed Network for Multimodal Experiential Representation of Concepts}}, 
author = {Tong, Jiaqing and Binder, Jeffrey R and Humphries, Colin and Mazurchuk, Stephen and Conant, Lisa L and Fernandino, Leonardo}, 
journal = {The Journal of Neuroscience}, 
issn = {0270-6474}, 
doi = {10.1523/jneurosci.1243-21.2022}, 
pmid = {35940877}, 
pmcid = {PMC9480893}, 
abstract = {{Neuroimaging, neuropsychological, and psychophysical evidence indicate that concept retrieval selectively engages specific sensory and motor brain systems involved in the acquisition of the retrieved concept. However, it remains unclear which supramodal cortical regions contribute to this process and what kind of information they represent. Here, we used representational similarity analysis of two large fMRI datasets with a searchlight approach to generate a detailed map of human brain regions where the semantic similarity structure across individual lexical concepts can be reliably detected. We hypothesized that heteromodal cortical areas typically associated with the default mode network encode multimodal experiential information about concepts, consistent with their proposed role as cortical integration hubs. In two studies involving different sets of concepts and different participants (both sexes), we found a distributed, bihemispheric network engaged in concept representation, composed of high-level association areas in the anterior, lateral, and ventral temporal lobe; inferior parietal lobule; posterior cingulate gyrus and precuneus; and medial, dorsal, ventrolateral, and orbital prefrontal cortex. In both studies, a multimodal model combining sensory, motor, affective, and other types of experiential information explained significant variance in the neural similarity structure observed in these regions that was not explained by unimodal experiential models or by distributional semantics (i.e., word2vec similarity). These results indicate that during concept retrieval, lexical concepts are represented across a vast expanse of high-level cortical regions, especially in the areas that make up the default mode network, and that these regions encode multimodal experiential information.SIGNIFICANCE STATEMENT Conceptual knowledge includes information acquired through various modalities of experience, such as visual, auditory, tactile, and emotional information. We investigated which brain regions encode mental representations that combine information from multiple modalities when participants think about the meaning of a word. We found that such representations are encoded across a widely distributed network of cortical areas in both hemispheres, including temporal, parietal, limbic, and prefrontal association areas. Several areas not traditionally associated with semantic cognition were also implicated. Our results indicate that the retrieval of conceptual knowledge during word comprehension relies on a much larger portion of the cerebral cortex than previously thought and that multimodal experiential information is represented throughout the entire network.}}, 
pages = {7121--7130}, 
number = {37}, 
volume = {42}, 
keywords = {}
}

@abstract{undefined, 
year = {2022}, 
author = {Tong, Jiaqing and Fernandino, Leonardo and Mazurchuk, Stephen and Conant, Lisa and Binder, Jeffrey}, 
title = {{A Common Neural Mechanism Underlying Object and Event Concept Representation}}, 
booktitle = {29th Annual Meeting of the Cognitive Neuroscience Society}, 
note = {29th Annual Meeting of the Cognitive Neuroscience Society}, 
keywords = {}, 
month = {6}
}